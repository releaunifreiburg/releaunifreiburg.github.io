---
layout: post
title:  "Deep Pipeline Embeddings for AutoML"
date:   2023-07-02 10:30:16 +0200
categories: jekyll update
permalink: /deepppipe/
google_analytics: G-K7M6YQS41Y
tags: [Pipeline-Optimization, AutoML, Scikit-Learn, Meta-Learning, Deep-Kernel-Learning]
---

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript" ></script>

This is a blog post about [our paper][our-paper] published on KDD 2023. Check the original paper and the [code][code] for further details. Some notation might differ from the original paper for the sake of simplicity.

## 1. Motivation

When using standard Machine Learning, practitioners need to make a lot of decisions such as which feature encoding algorithm or imputation method to use. Although deep learning helps reduce manual preprocessing, it is important to specify the optimizer and the architecture, among other aspects. We can abstract these machine learning systems as a **pipeline**, a group of stages that interact during training and inference. Each stage usually contains an algorithm \\(\mathcal{A}\\) and its hyperparameters \\( \lambda\\), which crucially affect the performance of the whole pipeline. 


{:refdef: style="text-align: center;"}
![alat text](/assets/images/deeppipe/standard_ml_pipeline.svg "Example of Standard ML Pipeline")
{: refdef}


{:refdef: style="text-align: center;"}
![alat text](/assets/images/deeppipe/deep_learning_pipeline.svg "Example of Deep Learning Pipeline")
{: refdef}

To attain the best performance possible, we need to search over the set of all possible pipelines, a.k.a. search space. Its dimensionality is however large, due to the almost limitless design choices. Consider a pipeline as depicted above with four stages, each of them has only 2 possible algorithms with one binary hyperparameter \\(\mathcal{A_i} \in \\{0,1\\}, \lambda _i \in \\{0,1\\} , i \in  \\{1,2,3,4\\}\\). This small set-up would produce a search space of dimensionality \\(8\\), but grows exponentially  as more flexible pipelines are considered. Thus, it is natural that ML engineers use handy AutoML tools that optimize the pipeline automatically. What if we perform the search over an embedding space that better captures the similarity between the pipelines? Our method **DeepPipe** creates this embedding space and performs the pipeline search on it!

{:refdef: style="text-align: center;"}
![alat text](/assets/images/deeppipe/original_and_embedding_space.svg "Original and Embedding Space")
{: refdef}

## 2. Creating the Embedding Network

We want to create a network that takes a pipeline \\(\mathbf{x}\\) in a high-dimensional original space and projects it to an embedding space \\( \tilde{\textbf{x}} = \phi(\mathbf{x}) \\), where similar pipelines are close to each other. However, how can we build such a network? We can build it via two methods: using assumptions and prior knowledge about the problem, a.k.a. inductive biases, or using meta-data.

{:refdef: style="text-align: center;"}
![alat text](/assets/images/deeppipe/embedding_network.svg)
{: refdef}

### 2.1. Embedding Network via Inductive Biases

What kind of prior knowledge about the pipelines can we encode in the architecture? A common prior assumption is that if two pipelines have similar components, i.e. the same algorithm for the estimation feature scaling or estimation, thus they should be considered similar. A nice recipe to implement this assumption into the network architecture is using separate smaller networks per stage. Firstly, take the feed-forward network to embed every algorithm's hyperparameters separately, and input the concatenation of these hyperparameter embeddings into a second feed-forward network. All this is assuming that only one algorithm is active per stage. The set of layers comprised by the networks embedding the hyperparameters are dubbed **encoder layers**, while the last part of the architecture is formed by the **aggregation layers**. In [our paper][our-paper] we demonstrate formally how this design produced the inductive biases discussed above.

{:refdef: style="text-align: center;"}
![alat text](/assets/images/deeppipe/deeppipe_network.svg "DeepPipe Embedding Network")
{: refdef}

### 2.2. Embedding Network via Meta-learning

{:refdef: style="text-align: center;"}
![alat text](/assets/images/deeppipe/creating_network.svg "Creating Embedding Network")
{: refdef}


Although our assumptions are well founded, leveraging meta-data from pipeline performances in similar tasks [^task] can provide additional insights. How can we use this meta-data about the performance across similar tasks? Firstly, we want to learn a similarity metric, so that similar pipelines lie closer in the embedding space. For this purpose, we use the method provided for Deep Kernel Learning [^2] (DKL) and Gaussian Processes, which has also been used for optimizing hyperparameters of single algorithms[^3]. Specifically, we use the performance (e.g. Validation Accuracy) to learn a kernel \\(k(\textbf{x}_i, \textbf{x}_j) \in [0,1] \\) that outputs a high value if the pipelines are similar. But how can we know if the two pipelines are similar given the meta-data? Two pipelines should be similar if they perform similarly in the same tasks! Given that we have \\(T\\) tasks in our training meta-data, we obtain the network and Gaussian process parameters \\(\gamma, \theta \\) by maximizing the marginal likelihood of the meta-data. For every task \\( t \\), it applies the kernel matrix on the evaluated pipelines \\(K^{t}\\), with components \\(k_i ,_j = k(\textbf{x}_i, \textbf{x}_j) \\), and pipeline performances as \\(y^{(t)}\\).


$$
    \text{min}_{\gamma, \theta} \; \sum_{t=1}^{T} \; {y^{(t)}}^{\mathrm{T}}{K^{(t)}}(\theta, \gamma)^{-1}{y^{(t)}}+\log\left|{K^{(t)}}(\theta, \gamma) \right|
$$


After the learning, two neighbor pipeline embeddings will likely have similar performance across all the tasks used on the meta-data. The performance of a pipeline may vary across tasks, as shown in the figure below by denoting high-performing pipelines with dark yellow. In a specific task, the pipelines are grouped in the embedding space, creating regions of high-performing and low-performing pipelines.


{:refdef: style="text-align: center;"}
![alat text](/assets/images/deeppipe/embeddings_per_task.svg "Embeddings per Task")
{: refdef}




### 2.3. Meta-learning vs Inductive Biases

It turns out that inductive biases are extremely helpful when there is little or no meta-data. Nevertheless, the meta-learning is very effective alone, if we have sufficient meta-data. Apparently, it is better to rely on the data than on our assumptions, given that we have enough data.


## 3. Pipeline Optimization with *DeepPipe* 

Once we create the embedding network, we can use it for exploring the embedding space and search for high-performing pipelines on a new task. The procedure performs 
Bayesian optimization [^4] using the embedding network and the Gaussian Process, also called Deep Kernel Gaussian Process. A detailed explanation of the process falls out of the scope, but we depict below an intuition of the process. In the beginning, we observe the true performance of some random pipelines. By using the Deep Kernel, we predict the performance of unobserved pipelines and uncover regions of (potentially) high-performing pipelines (red and green pipelines). However, we also need to balance the uncertainty in our predictions, as they can be wrong for pipelines located far from the observed ones (red), while we have more certainty about the performance of certain pipelines (green). 

{:refdef: style="text-align: center;"}
![alat text](/assets/images/deeppipe/optimizing_pipelines.svg "Optimizing Pipelines")
{: refdef}


## 4. Results

Optimization with *DeepPipe* is very competitive and achieves state-of-the-art compared with other methods. Besides, when non-using meta-data, the inductive biases are really helpful, *DeepPipe* embedding network with 1 and 2 encoder layers as shown below.


{:refdef: style="text-align: center;"}
![alat text](/assets/images/deeppipe/deeppipe_results1.png "Results non-transfer set-up")
{: refdef}

When using meta-data (transfer pipeline optimization), *DeepPipe* also attains excelent results.

{:refdef: style="text-align: center;"}
![alat text](/assets/images/deeppipe/deeppipe_results2.png "Results transfer set-up")
{: refdef}


## 5. Installation and Usage

You can try *DeepPipe* for optimizing scikit-learn pipelines by installing our API. Further details on experiments can be found in our [code] base.


{% highlight bash %}
conda create -n deeppipe_env python==3.9
pip install deeppipe_api
{% endhighlight %}


{% highlight python %}
from deeppipe_api.deeppipe import load_data, openml, DeepPipe

task_id = 37
task = openml.tasks.get_task(task_id)
X_train, X_test, y_train, y_test = load_data(task, fold=0)
deep_pipe = DeepPipe(n_iters = 50,  #bo iterations
                    time_limit = 3600 #in seconds
                    )
deep_pipe.fit(X_train, y_train)
y_pred = deep_pipe.predict(X_test)

#Test
score = deep_pipe.score(X_test, y_test)
print("Test acc.:", score)

#print best pipeline
print(deep_pipe.model)
{% endhighlight %}


## References

[^task]: In this context, a task is a ML problem and the dataset. For instance a class prediction on Iris Dataset or binary classification on Titanic Dataset are two different tasks. 
[^2]: [Deep Kernel Learning](https://arxiv.org/abs/1511.02222)
[^3]: [FSBO](https://arxiv.org/abs/2101.07667) 
[^4]: [Bayesian Optimization Blog Post](http://krasserm.github.io/2018/03/21/bayesian-optimization/)

[our-paper]: https://arxiv.org/abs/2305.14009
[code]: https://github.com/releaunifreiburg/DeepPipe

